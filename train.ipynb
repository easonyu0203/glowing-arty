{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Train sample notebook\n",
    "\n",
    "## Setting\n",
    "set up all hyperparameters and configurations here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "DATA_ROOT = \"data\"\n",
    "MODEL_PATH = \"model.pth\"\n",
    "\n",
    "class Hyperparameters:\n",
    "    def __init__(self):\n",
    "        # Data\n",
    "        self.val_ratio = 0.2\n",
    "\n",
    "        # Training hyperparameters\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 600\n",
    "\n",
    "        # Optimizer & LR scheme\n",
    "        self.opt_name = 'sgd'\n",
    "        self.momentum = 0.9\n",
    "        self.lr = 0.5\n",
    "        self.lr_scheduler_name = 'cosine'\n",
    "        self.lr_warmup_epochs = 5\n",
    "        self.lr_warmup_method = 'linear'\n",
    "        self.lr_warmup_decay = 0.01\n",
    "\n",
    "        # Regularization and Augmentation\n",
    "        self.weight_decay = 2e-05\n",
    "        self.norm_weight_decay = 0.0\n",
    "        self.label_smoothing = 0.1\n",
    "\n",
    "        # Resizing\n",
    "        self.val_crop_size = 224\n",
    "        self.train_crop_size = 176\n",
    "\n",
    "    def to_dict(self):\n",
    "        return self.__dict__\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import & Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available device: NVIDIA GeForce RTX 3070\n"
     ]
    },
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import utils; importlib.reload(utils)\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms as T\n",
    "from model import ResNet50Wrapper\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from typing import Tuple\n",
    "\n",
    "DEVICE = utils.torch.detect_device(verbose=True)\n",
    "torch.device(DEVICE)\n",
    "# start a new experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\n",
    "def setup_training(config: Hyperparameters) -> Tuple[DataLoader, DataLoader, nn.Module, nn.Module, torch.optim.Optimizer, torch.optim.lr_scheduler._LRScheduler]:\n",
    "    print(\"splitting dataset...\")\n",
    "    train_dir, val_dir = utils.data.split_dataset(DATA_ROOT, val_ratio=config.val_ratio)\n",
    "\n",
    "    print(\"Loading training data...\")\n",
    "    train_preprocess = T.Compose([\n",
    "        T.Resize([232, ]),\n",
    "        T.CenterCrop(config.train_crop_size),\n",
    "        T.PILToTensor(),\n",
    "        T.ConvertImageDtype(torch.float),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    train_dataset = ImageFolder(root=train_dir, transform=train_preprocess)\n",
    "\n",
    "    print(\"Loading validation data...\")\n",
    "    val_preprocess = T.Compose([\n",
    "        T.Resize([232, ]),\n",
    "        T.CenterCrop(config.train_crop_size),\n",
    "        T.PILToTensor(),\n",
    "        T.ConvertImageDtype(torch.float),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    val_dataset = ImageFolder(root=val_dir, transform=val_preprocess)\n",
    "\n",
    "    print(\"Creating data loaders...\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=16, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "    print(\"Creating model...\")\n",
    "    model = ResNet50Wrapper(num_classes=len(train_dataset.classes))\n",
    "\n",
    "    print(\"Creating criterion...\")\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=config.label_smoothing)\n",
    "\n",
    "    print(\"Creating optimizer...\")\n",
    "    if config.opt_name != 'sgd': raise NotImplementedError(\"Only SGD is supported\")\n",
    "    parameters = utils.train.set_weight_decay(\n",
    "        model,\n",
    "        config.weight_decay,\n",
    "        norm_weight_decay=config.norm_weight_decay\n",
    "    )\n",
    "    optimizer = torch.optim.SGD(\n",
    "        parameters,\n",
    "        lr=config.lr,\n",
    "        momentum=config.momentum,\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "\n",
    "    print(\"Creating learning rate scheduler...\")\n",
    "    if config.lr_scheduler_name != 'cosine': raise NotImplementedError(\"Only cosine is supported\")\n",
    "    main_lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        config.epochs - config.lr_warmup_epochs,\n",
    "        eta_min=0\n",
    "    )\n",
    "\n",
    "    if config.lr_warmup_epochs > 0:\n",
    "        if config.lr_warmup_method != 'linear': raise NotImplementedError(\"Only linear is supported\")\n",
    "        warmup_lr_scheduler = optim.lr_scheduler.LinearLR(\n",
    "            optimizer,\n",
    "            start_factor=config.lr_warmup_decay,\n",
    "            total_iters=config.lr_warmup_epochs\n",
    "        )\n",
    "        lr_scheduler = optim.lr_scheduler.SequentialLR(\n",
    "                optimizer, schedulers=[warmup_lr_scheduler, main_lr_scheduler], milestones=[config.lr_warmup_epochs]\n",
    "            )\n",
    "    else:\n",
    "        lr_scheduler = main_lr_scheduler\n",
    "\n",
    "    return train_loader, val_loader, model, criterion, optimizer, lr_scheduler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, criterion, optimizer, data_loader, epoch):\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(data_loader):\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # wandb log every 10 batches\n",
    "        if i % 10 == 0:\n",
    "            example_ct = i * len(images) + epoch * len(data_loader.dataset)\n",
    "            acc1, acc3 = utils.train.accuracy(outputs, labels, topk=(1, 3))\n",
    "            loss_value = loss.item()\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            wandb.log({\n",
    "                \"train/loss\": loss_value,\n",
    "                \"train/acc1\": acc1,\n",
    "                \"train/acc3\": acc3,\n",
    "                \"train/lr\": lr,\n",
    "                \"train/epoch\": epoch,\n",
    "            }, step=example_ct)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, criterion, data_loader, example_ct):\n",
    "    model.eval()\n",
    "    acc1_avg, acc3_avg, loss_avg = utils.train.AverageMeter(), utils.train.AverageMeter(), utils.train.AverageMeter()\n",
    "    with torch.inference_mode():\n",
    "        for i, (images, labels) in enumerate(data_loader):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # calculate metrix\n",
    "            acc1, acc3 = utils.train.accuracy(outputs, labels, topk=(1, 3))\n",
    "            loss = loss.item()\n",
    "            acc1_avg.update(acc1, images.size(0))\n",
    "            acc3_avg.update(acc3, images.size(0))\n",
    "            loss_avg.update(loss, images.size(0))\n",
    "\n",
    "        # log to wandb\n",
    "        wandb.log({\n",
    "            \"val/loss\": loss_avg.avg,\n",
    "            \"val/acc1\": acc1_avg.avg,\n",
    "            \"val/acc3\": acc3_avg.avg,\n",
    "        }, step=example_ct)\n",
    "\n",
    "        # random sample 10 images for data_loader\n",
    "        sample_indices = np.random.choice(len(data_loader), 10, replace=False)\n",
    "        for i in sample_indices:\n",
    "            images, labels = data_loader.dataset[i]\n",
    "            images = images.unsqueeze(0).to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            preds = preds.item()\n",
    "            label = data_loader.dataset.classes[labels]\n",
    "            pred = data_loader.dataset.classes[preds]\n",
    "            # get pred confidence\n",
    "            pred_confidence = torch.softmax(outputs, dim=1)[0, preds].item()\n",
    "            wandb.log({\n",
    "                \"val/sample\": [wandb.Image(images[0], caption=f\"pred: {pred}({pred_confidence:.2f}), label: {label}\")]\n",
    "            }, step=example_ct)\n",
    "\n",
    "    return acc1_avg.avg, acc3_avg.avg, loss_avg.avg\n",
    "\n",
    "\n",
    "def pipline(hyperparameters: Hyperparameters):\n",
    "    with wandb.init(project=\"new-sota-model\", config=hyperparameters.to_dict()):\n",
    "        config: Hyperparameters = wandb.config\n",
    "        train_loader, val_loader, model, criterion, optimizer, lr_scheduler = setup_training(config)\n",
    "\n",
    "        print(\"Start training...\")\n",
    "        wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "        for epoch in tqdm(range(config.epochs)):\n",
    "             train_one_epoch(model, criterion, optimizer, train_loader, epoch)\n",
    "             lr_scheduler.step()\n",
    "             example_ct = (epoch + 1) * len(train_loader.dataset)\n",
    "             acc1, acc3, loss = evaluate(model, criterion, val_loader, example_ct)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def main():\n",
    "    hyperparameters = Hyperparameters()\n",
    "    pipline(hyperparameters)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting dataset...\n",
      "Aborting.\n",
      "Loading training data...\n",
      "Loading validation data...\n",
      "Creating data loaders...\n",
      "Creating model...\n",
      "Creating criterion...\n",
      "Creating optimizer...\n",
      "Creating learning rate scheduler...\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, model, criterion, optimizer, lr_scheduler = setup_training(hyperparameters)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
